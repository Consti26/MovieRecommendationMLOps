#version: '3.9'

services:
  database:
    build: 
      context: "${SCRIPT_DIR}/data"
      dockerfile: "dockerfile"
      args:
        DATABASE_PORT: ${DATABASE_PORT} # This is available at build time
    container_name: "database_container"
    ports:
      - "${DATABASE_PORT}:${DATABASE_PORT}" # Expose the database API port
    volumes:
      - database_raw_volume:/home/api_database/raw_data
      - database_processed_volume:/home/api_database/processed_data
    env_file:
      - ".env" # This is only available at runtime
    networks: 
      - "movie_recommendation_network"
  mlflow:
    build: 
      context: "${SCRIPT_DIR}/models/mlflow"
      dockerfile: "dockerfile"
      args:
        MLFLOW_PORT: ${MLFLOW_PORT} # This is available at build time
    container_name: "mlflow_container"
    volumes:
      - mlflow_data:/mlflow
    env_file:
      - ".env"
    ports:
      - "${MLFLOW_PORT}:${MLFLOW_PORT}"
    networks:
      - "movie_recommendation_network"
    restart: always
  preprocessing:
    build: 
      context: "${SCRIPT_DIR}/features"
      dockerfile: "dockerfile"
      args:
        PREPROCESSING_PORT: ${PREPROCESSING_PORT}
    container_name: "preprocess_container"
    networks: 
      - "movie_recommendation_network"
    ports:
      - "${PREPROCESSING_PORT}:${PREPROCESSING_PORT}"
    env_file:
      - ".env"
  training:
    build: 
      context: "${SCRIPT_DIR}/models"
      dockerfile: "${SCRIPT_DIR}/models/training/dockerfile"
      args:
        TRAINING_PORT: ${TRAINING_PORT}
    container_name: "training_container"
    depends_on:
      - "database"
      - "mlflow"
    volumes:
      - mlflow_data:/mlflow
    env_file:
      - ".env"
    ports:
      - "${TRAINING_PORT}:${TRAINING_PORT}"
    networks:
      - "movie_recommendation_network"
  inference:
    build: 
      context: "${SCRIPT_DIR}/models"
      dockerfile: "${SCRIPT_DIR}/models/inference/dockerfile"
      args:
        INFERENCE_PORT: ${INFERENCE_PORT}
    container_name: "inference_container"
    depends_on:
      - "mlflow"
    volumes:
      - mlflow_data:/mlflow
    env_file:
      - ".env"
    ports:
      - "${INFERENCE_PORT}:${INFERENCE_PORT}"
    networks:
      - "movie_recommendation_network"

volumes:
  mlflow_data:
    driver: local
    driver_opts:
      type: none
      device: "${SCRIPT_DIR}/models/mlflow/workdir"
      o: bind
  database_raw_volume:
    driver: local
    driver_opts:
      type: none
      device: "${SCRIPT_DIR}/data/raw_data"
      o: bind
  database_processed_volume:
    driver: local
    driver_opts:
      type: none
      device: "${SCRIPT_DIR}/data/processed_data"
      o: bind

networks:
  movie_recommendation_network:
    #driver: bridge
    external: true